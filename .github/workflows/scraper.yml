name: LinkedIn Scraper Bot

on:
  # Run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      search_query:
        description: 'Custom search query (e.g., site:linkedin.com/in/ data scientist)'
        required: false
        default: 'site:linkedin.com/in/ software engineer'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y wget gnupg
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome.gpg
          echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version
      
      - name: Install ChromeDriver
        run: |
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+')
          echo "Chrome version: $CHROME_VERSION"
          wget -q "https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$(curl -s https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_STABLE)/linux64/chromedriver-linux64.zip" -O chromedriver.zip || \
          wget -q "https://storage.googleapis.com/chrome-for-testing-public/$(curl -s https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_STABLE)/linux64/chromedriver-linux64.zip" -O chromedriver.zip
          unzip -o chromedriver.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          chromedriver --version
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run scraper
        env:
          HEADLESS: 'true'
          SEARCH_QUERY: ${{ github.event.inputs.search_query || 'site:linkedin.com/in/ software engineer' }}
          CHROME_BIN: /usr/bin/google-chrome
        run: python bot.py
        continue-on-error: true
      
      - name: Upload results as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: linkedin-profiles-${{ github.run_number }}
          path: linkedin_profiles.csv
          retention-days: 30
          if-no-files-found: ignore
      
      - name: Commit results to repo
        if: always()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add linkedin_profiles.csv || true
          git diff --staged --quiet || git commit -m "ðŸ“Š Update scraped profiles - $(date +'%Y-%m-%d %H:%M:%S')"
          git push || true
