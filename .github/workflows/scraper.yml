name: LinkedIn Scraper Bot

on:
  # Run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      search_query:
        description: 'Custom search query (e.g., site:linkedin.com/in/ data scientist)'
        required: false
        default: 'site:linkedin.com/in/ software engineer'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run scraper
        env:
          HEADLESS: 'true'
          SEARCH_QUERY: ${{ github.event.inputs.search_query || 'site:linkedin.com/in/ software engineer' }}
        run: python bot.py
      
      - name: Upload results as artifact
        uses: actions/upload-artifact@v4
        with:
          name: linkedin-profiles-${{ github.run_number }}
          path: linkedin_profiles.csv
          retention-days: 30
      
      - name: Commit results to repo
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add linkedin_profiles.csv || true
          git diff --staged --quiet || git commit -m "ðŸ“Š Update scraped profiles - $(date +'%Y-%m-%d %H:%M:%S')"
          git push || true
